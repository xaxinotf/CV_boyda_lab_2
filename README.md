# CV_boyda_lab_2 **Лабораторна робота 2. Чебан Богдан ТТП-42**

Предмет:**Розпізнавання жестів з використанням нейромереж**

## Опис проекту

Я розробив лабораторну роботу з комп'ютерного зору, яка використовує Python, OpenCV, MediaPipe, NumPy, Matplotlib і Pandas для розпізнавання жесту «Б» (Українська дактильна абетка). У проекті реалізовано:
- Виявлення рук за допомогою MediaPipe.
- Розрахунок коефіцієнта відкритості руки для класифікації жесту.
- Динамічне налаштування порогового значення через trackbar.
- Запис даних: лог подій, дані по кадрах (gesture_data_log.csv), координати кожного landmark (landmarks_data_log.csv).
- Формування підсумкового звіту (gesture_report.txt) із загальними показниками.
- Візуалізацію графіка стабільності розпізнавання в режимі реального часу.

## Вимоги

Для запуску цього проекту мені знадобилися:
- Python 3.10 (або новіша версія)
- OpenCV-Python
- MediaPipe
- NumPy
- Matplotlib
- Pandas

## Інструкції по встановленню та запуску

### 1. Клонування репозиторію

Я клоную репозиторій до свого локального комп'ютера за допомогою наступної команди:

```bash
git clone https://github.com/xaxinotf/CV_boyda_lab_2.git
cd CV_boyda_lab_2
```

### 2. Створення віртуального середовища (рекомендовано)

Я рекомендую створити віртуальне середовище для ізоляції залежностей:

```bash
python -m venv venv
```

Після створення активую середовище:
- **На Windows:**

  ```bash
  venv\Scripts\activate
  ```

- **На Linux/Mac:**

  ```bash
  source venv/bin/activate
  ```

### 3. Встановлення залежностей

Я встановлюю необхідні бібліотеки, виконавши:

```bash
pip install opencv-python mediapipe numpy matplotlib pandas
```

*Якщо є файл requirements.txt, я можу встановити залежності командою:*

```bash
pip install -r requirements.txt
```

### 4. Запуск проекту

Щоб запустити лабораторну роботу, я виконую команду:

```bash
python main.py
```

Після запуску відкривається вікно з відеопотоком з вебкамери, де:
- Розпізнається жест «Б» на основі коефіцієнта відкритості руки.
- Можна регулювати порогове значення (Openness Thresh) через trackbar.
- В режимі реального часу відображається інформація (FPS, коефіцієнт відкритості, схожість до «Б», стабільність).
- Дані зберігаються у CSV-файлах, а також формується підсумковий звіт у gesture_report.txt.

### 5. Звітність

Після завершення роботи проекту я отримую:
- **gesture.log** – лог подій.
- **gesture_data_log.csv** – дані про кожен кадр (час, FPS, класифікація, коефіцієнт відкритості, схожість).
- **landmarks_data_log.csv** – координати кожного landmark для кожного кадру.
- **gesture_report.txt** – підсумковий звіт із середніми показниками (FPS, коефіцієнт відкритості, схожість, розподіл жестів).

## Заключення

Я створив цей проект, щоб продемонструвати можливості розпізнавання жестів з розширеною звітністю. За допомогою даного репозиторію я можу аналізувати ефективність роботи системи, налагоджувати параметри в режимі реального часу та використовувати зібрані дані для подальшого дослідження.



**Вимоги:**

1. **Розпізнавання жесту "Б":**  
   Я використовую MediaPipe Hands для отримання координат рук та обчислення коефіцієнта відкритості руки. За допомогою цього коефіцієнта (ratio) я класифікую жест: якщо ratio менше заданого порогу, то я вважаю, що це жест "Б", інакше – "Не Б". Також я обчислюю "схожість" до літери "Б", що відображається у відсотках.

2. **Налаштування порогу в режимі реального часу:**  
   Я додав trackbar у вікні "Відео", який дозволяє динамічно регулювати порогове значення відкритості (openness_threshold). Це дозволяє коригувати параметри класифікації без перезапуску програми.

3. **Логування подій:**  
   Результати розпізнавання, значення коефіцієнта відкритості, схожість, а також інформацію про стабільність розпізнавання я логую у файл `gesture.log`.

4. **Запис даних у CSV:**  
   Я зберігаю детальну інформацію про кожен кадр (час, FPS, класифікований жест, коефіцієнт відкритості та схожість) у CSV-файл `gesture_data_log.csv`. Це дозволяє мені пізніше проаналізувати дані.

5. **Збереження координат кожного landmark:**  
   Для кожного кадру, де виявлено руку, я записую координати кожного landmark (x, y, z) у окремий CSV-файл `landmarks_data_log.csv`. Це допомагає провести глибший просторовий аналіз позицій рук.

6. **Підсумкова звітність:**  
   Після завершення роботи я використовую бібліотеку Pandas для формування підсумкового звіту (середній FPS, середній коефіцієнт відкритості, середня схожість, розподіл розпізнаних жестів). Цей звіт зберігається у файлі `gesture_report.txt` і виводиться у консоль.

7. **Візуалізація результатів:**  
   Я використовую matplotlib для побудови графіку стабільності розпізнавання у режимі реального часу. Це дозволяє мені відслідковувати, наскільки послідовно працює система протягом сесії.

